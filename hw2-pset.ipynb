{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "898baed3",
   "metadata": {},
   "source": [
    "# HW 2: Efficient Fine-Tuning with BitFit?\n",
    "**Due: February 24, 9:30 AM**\n",
    "\n",
    "In this homework assignment, you will replicate [the BitFit experiments (Zaken et al., 2020)](https://aclanthology.org/2022.acl-short.1/), whose paper we read and analyzed together in class. You will first use the [ðŸ¤— Transformers framework](https://huggingface.co/docs/transformers/index) to fine-tune a [BERT$_\\text{tiny}$ model](https://huggingface.co/prajjwal1/bert-tiny) ([Turc et al., 2019](https://arxiv.org/abs/1908.08962); [Bhargava et al., 2021](https://aclanthology.org/2021.insights-1.18/)) on the IMDb dataset. You will then fine-tune the same model, but with all parameters frozen other than the bias terms. You will compare the two models on the following metrics: (1) their accuracy on the IMDb test set and (2) the number of parameters trained during fine-tuning.\n",
    "\n",
    "## Important: Read Before Starting\n",
    "\n",
    "In the following exercises, you will need to implement functions defined in the `train_model.py` and `test_model.py` scripts. **Please write all your code in those files.** You should not submit this notebook with your solutions, and we will not grade it if you do. Please be aware that code written in a Jupyter notebook may run differently when copied into Python modules.\n",
    "\n",
    "The outputs shown in this notebook are the outputs that you should get **when all problems have been completed correctly**. You may obtain different results if you attempt to run the code cells before you have completed the problem set, or if you have completed one or more problems incorrectly.\n",
    "\n",
    "For part of this assignment, you will be asked to fine-tune a BERT$_\\text{tiny}$ model on the IMDb dataset with hyperparameter tuning. **This will take several hours to run on a laptop with a CPU.** You may want to instead run your code on [Google Colaboratory](https://colab.research.google.com/) using a free GPU.\n",
    "\n",
    "To begin, please run the following `import` statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2593b3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from collections.abc import Iterable\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Model and tokenizer from ðŸ¤— Transformers\n",
    "from transformers import AutoModelForSequenceClassification, \\\n",
    "    BertForSequenceClassification, BertTokenizerFast\n",
    "\n",
    "# Code you will write for this assignment\n",
    "from train_model import init_model, preprocess_dataset, init_trainer\n",
    "from test_model import init_tester"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5110c1a8",
   "metadata": {},
   "source": [
    "## Problem 1: Setup (30 Points in Total)\n",
    "\n",
    "In this assignment, you will fine-tune a pre-trained Transformer model using libraries provided by [Hugging Face](https://huggingface.co/) (whose name is usually styled using the emoji ðŸ¤—). You have already been exposed to Hugging Face in lab, where you used the [ðŸ¤— Datasets](https://huggingface.co/docs/datasets/index) library to load the IMDb dataset and the [ðŸ¤— Transformers](https://huggingface.co/docs/transformers/index) library to load a pre-trained BERT$_\\text{tiny}$ model. In the following problems, additionally use the [ðŸ¤— Evaluate](https://huggingface.co/docs/evaluate/index) library, which provides evaluation metrics such as accuracy and F1.\n",
    "\n",
    "For several parts of this problem, you will need to refer to the [Hugging Face fine-tuning tutorial](https://huggingface.co/docs/transformers/training) for guidance.\n",
    "\n",
    "### Problem 1a: Understand the ðŸ¤— Transformers Library (No Submission, 0 Points)\n",
    "\n",
    "ðŸ¤— Transformers is imported into Python via the name `transformers`. Please find the import statements from ðŸ¤— Transformers in the code cell above.\n",
    "\n",
    "ðŸ¤— Transformers comes with a number of different Transformer architectures, as well as [the Model Hub, a repository of pre-trained model parameters](https://huggingface.co/models). A pre-trained model is loaded by calling the model architecture's `.from_pretrained` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e14fd3b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5969e2b8b5d244ce8fdddd142c042dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5165047bcb6e456c930521d163be15d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)\"pytorch_model.bin\";:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f653226",
   "metadata": {},
   "source": [
    "The code above loads a Transformer classifier consisting of a pre-trained BERT$_\\text{base}$ encoder with case-insensitive vocabulary and a randomly initialized 2-layer MLP decoder with tanh activation. The choice of this particular set of pre-trained parameters is specified by the identifier `'bert-base-uncased'`, which is passed to the first parameter of `.from_pretrained`. Different pre-trained weights can be loaded by passing a different identifier to `.from_pretrained`. The following code loads the BERT$_\\text{tiny}$ model from [Turc et al. (2019)](https://arxiv.org/abs/1908.08962) and [Bhargava et al. (2021)](https://aclanthology.org/2021.insights-1.18/), which you will be fine-tuning in this assignment. (The `/` indicates that this is a user-submitted model, uploaded by the user [`prajjwal1`](https://huggingface.co/prajjwal1).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f491da1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1304db51c28c4d448b4947ae8ba7a45b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/285 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a94ea7228154c118cde18906a28624e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)\"pytorch_model.bin\";:   0%|          | 0.00/17.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"prajjwal1/bert-tiny\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbed54d",
   "metadata": {},
   "source": [
    "In order to load a model using the code above, you would have to know that BERT$_{\\text{tiny}}$'s architecture is implemented using the same class as BERT$_{\\text{base}}$. This is not true in general, however. For instance, if you wanted to initialize a RoBERTa classifier instead of a BERT classifier, you would need to call `RobertaForSequenceClassification.from_pretrained` instead of `BertForSequenceClassification.from_pretrained`. When you don't know which class implements the architecture of pre-trained model you want to load, you can use the `AutoModelForSequenceClassification` class ([and equivalent classes for other tasks](https://huggingface.co/docs/transformers/model_doc/auto)), which will figure out which class to instantiate based on the pre-trained weights you would like to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33cf7838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# This code does exactly the same thing as the previous code cell\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"prajjwal1/bert-tiny\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b9706",
   "metadata": {},
   "source": [
    "In addition to models, ðŸ¤— Transformers also provides tokenizers that implement a full processing pipeline similar to what you implemented in HW 2. You can load the appropriate tokenizer for your model using a `.from_pretrained` method, just as you did with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5c374e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7393dca83b6c4ae98fde1e1e6f73acfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"prajjwal1/bert-tiny\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf262f25",
   "metadata": {},
   "source": [
    "As we saw in lab, the tokenizer object can be called as a function. Doing so will return a fully processed input, ready to be passed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2388ba95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 7592, 2088,  999,  102,    0],\n",
       "        [ 101, 2129, 2024, 2017, 1029,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because ðŸ¤— Transformers supports multiple deep learning libraries, you will\n",
    "# need to use the keyword parameter return_tensors in order to indicate that\n",
    "# you want your inputs to be returned in PyTorch format.\n",
    "inputs = tokenizer([\"Hello world!\", \"How are you?\"], padding=True, \n",
    "                   return_tensors=\"pt\")\n",
    "inputs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f381cc9",
   "metadata": {},
   "source": [
    "The inputs returned by the tokenizer are passed to the model via [dictionary unpacking](https://realpython.com/python-kwargs-and-args/). The output of the model is structured, with various kinds of information provided depending on keyword arguments passed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63f1cba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.2078,  0.0662],\n",
      "        [-0.0285, -0.0969]]), hidden_states=None, attentions=None)\n",
      "\n",
      "tensor([[ 0.2078,  0.0662],\n",
      "        [-0.0285, -0.0969]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "print(outputs, end=\"\\n\\n\")\n",
    "\n",
    "# Use the dot operator to access parts of the output\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de67f0e9",
   "metadata": {},
   "source": [
    "### Problem 1b: Understand BERT Inputs (Written, 10 Points)\n",
    "\n",
    "Look at the tokenized inputs from two code cells above. The inputs are represented as a dict with three keys: `'input_ids'`, `'token_type_ids'`, and `'attention_mask'`. What do each of those three inputs represent? Please consult the [original BERT paper (Devlin et al., 2018)](https://arxiv.org/abs/1810.04805) for guidance.\n",
    "\n",
    "### Problem 1c: Understand BERT Hyperparameters (Written, 10 Points)\n",
    "\n",
    "For this assignment, you will perform hyperparameter tuning for the BERT$_\\text{tiny}$ model using the same procedure as in the [original paper](https://arxiv.org/abs/1908.08962). Their hyperparameter tuning procedure is documented in the [official BERT GitHub repository](https://github.com/google-research/bert) under the heading \"**\\*\\*\\*\\*\\*New March 11th, 2020: Smaller BERT Models\\*\\*\\*\\*\\***.\" Please read this documentation and describe how hyperparameter tuning was performed for the GLUE benchmark.\n",
    "\n",
    "### Problem 1d: Prepare Dataset (Code, 10 Points)\n",
    "\n",
    "As in lab, we will be using the IMDb dataset provided by ðŸ¤— Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "406d2800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53958f1d6d4e4abcbac298f1c318c8ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.31k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2388405d7644ea8c5022a0a2a6d742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e03f7a527d4f05bef2933f74f49590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imdb/plain_text to PATH REDACTED...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17583f36f24e42ae81d422ec19ba7235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imdb downloaded and prepared to PATH REDACTED. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5276d4d2c14e048d07a8d350a5adf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load IMDb dataset and create validation split\n",
    "imdb = load_dataset(\"imdb\")\n",
    "split = imdb[\"train\"].train_test_split(.2, seed=3463)\n",
    "imdb[\"train\"] = split[\"train\"]\n",
    "imdb[\"val\"] = split[\"test\"]\n",
    "del imdb[\"unsupervised\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8cb0e7",
   "metadata": {},
   "source": [
    "The ðŸ¤— Transformers fine-tuning API expects datasets to be pre-processed through the following steps.\n",
    "- All input texts should be tokenized.\n",
    "- BERT models have a maximum input length, and all inputs need to be truncated to this length.\n",
    "- Inputs shorter than the maximum input length should be padded to this length.\n",
    "- The pre-processed inputs do not need to be in the form of PyTorch tensors.\n",
    "\n",
    "These steps are performed by the `preprocess_dataset` function in `run_experiment.py`, which you will implement for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efacd457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e326ae5497c2472782f9019e522d26db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ec9167a0b742ec8ead53489d585798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e12501a81f84095b92e798bc6065b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:\n",
      "<class 'list'>\n",
      "['As so many others ha', 'When converting a bo']\n",
      "\n",
      "label:\n",
      "<class 'list'>\n",
      "[1, 0]\n",
      "\n",
      "input_ids:\n",
      "<class 'list'>\n",
      "[[101, 2004, 2061, 2116, 2500, 2031, 2517, 1010, 2023, 2003, 1037, 6919, 4516, 1012, 2182, 2003, 1037, 2862, 1997, 1996], [101, 2043, 16401, 1037, 2338, 2000, 2143, 1010, 2009, 2003, 3227, 1037, 2204, 2801, 2000, 2562, 2012, 2560, 2070, 1997]]\n",
      "\n",
      "token_type_ids:\n",
      "<class 'list'>\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "\n",
      "attention_mask:\n",
      "<class 'list'>\n",
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imdb[\"train\"] = preprocess_dataset(imdb[\"train\"], tokenizer)\n",
    "imdb[\"val\"] = preprocess_dataset(imdb[\"val\"], tokenizer)\n",
    "imdb[\"test\"] = preprocess_dataset(imdb[\"test\"], tokenizer)\n",
    "\n",
    "# Visualize the preprocessed dataset\n",
    "for k, v in imdb[\"val\"][:2].items(): \n",
    "    print(\"{}:\\n{}\\n{}\\n\".format(k, type(v),\n",
    "                                 [item[:20] if isinstance(item, Iterable) else \n",
    "                                 item for item in v[:5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac12ac",
   "metadata": {},
   "source": [
    "Please base your implementation on the [Hugging Face fine-tuning tutorial](https://huggingface.co/docs/transformers/training), and please consult [Appendix A.2 of the BERT paper](https://arxiv.org/abs/1810.04805) to find out what the maximum input length should be.\n",
    "\n",
    "## Problem 2: Implement Experiment (50 Points in Total)\n",
    "### Problem 2a: Freeze Non-Bias Weights (Code, 10 Points)\n",
    "\n",
    "At the end of this assignment, you will be comparing a BERT$_{\\text{tiny}}$ model fine-tuned using BitFit to a BERT$_{\\text{tiny}}$ model fine-tuned _without_ BitFit. To run that experiment, you will need to support freezing all non-bias parameters of the model. To do this, please implement the `init_model` function, illustrated below. This function should load a pre-trained BERT classifier model from the Hugging Face Model Hub and optionally freeze non-bias parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8727100e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# The first parameter is unused; we just pass None to it\n",
    "model = init_model(None, \"prajjwal1/bert-tiny\", use_bitfit=True)\n",
    "\n",
    "# Check if weight matrix is frozen\n",
    "print(model.bert.encoder.layer[0].attention.self.query.weight.requires_grad)\n",
    "\n",
    "# Check if bias term is frozen\n",
    "print(model.bert.encoder.layer[0].attention.self.query.bias.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173b6886",
   "metadata": {},
   "source": [
    "**Hint:** Please consult the [documentation for the function `nn.Module.named_parameters`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_parameters).\n",
    "\n",
    "### Problem 2b: Set Up Trainer and Tester (Code, 20 Points)\n",
    "\n",
    "ðŸ¤— Transformers provides a [`Trainer` object](https://huggingface.co/docs/transformers/main_classes/trainer) that implements training and testing a neural network. For this problem, please implement the functions `init_trainer` in `train_model.py` and `init_tester` in `test_model.py`, which will set up the `Trainer`s used to train and test your model, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06758f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at PATH REDACTED\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at PATH REDACTED\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file checkpoints/run-13/checkpoint-1252/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/run-13/checkpoint-1252/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at checkpoints/run-13/checkpoint-1252.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# Creates a Trainer from a Hugging Face Model Hub identifier\n",
    "trainer = init_trainer(\"prajjwal1/bert-tiny\", imdb[\"train\"], imdb[\"val\"])\n",
    "\n",
    "# Creates a Trainer to test a Hugging Face saved model\n",
    "tester = init_tester(\"checkpoints/run-13/checkpoint-1252\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67fcde8",
   "metadata": {},
   "source": [
    "Your `init_trainer` function needs to support the following.\n",
    "- The training configuration (total number of epochs, early stopping criteria if any) must match your answer for Problem 1c.\n",
    "- Your `Trainer` needs to save the model obtained during each training run to a folder called `checkpoints`.\n",
    "- You should leave the `model` keyword parameter blank and instead pass an argument to the `model_init` keyword parameter.\n",
    "- It should evaluate models based on accuracy.\n",
    "\n",
    "Your `init_tester` function needs to support the following.\n",
    "- The `Trainer` should only support testing and not traiing.\n",
    "- It should evaluate models based on accuracy.\n",
    "\n",
    "\n",
    "Please use the [Hugging Face fine-tuning tutorial](https://huggingface.co/docs/transformers/training) as well as [this forum post](https://discuss.huggingface.co/t/using-trainer-at-inference-time/9378/3) for guidance. You may need to create new functions for this problem, and you may find it useful to learn about [lambda expressions](https://realpython.com/python-lambda/) if you don't know about them already.\n",
    "\n",
    "### Problem 2c: Set Up Hyperparameter Tuning (Code, 20 Points)\n",
    "\n",
    "Finally, to complete the experiment setup, you will implement hyperparameter tuning using the [Optuna](https://optuna.org/) framework. Optuna is integrated with ðŸ¤— Transformers, and it can be invoked via the `Trainer.hyperparameter_search` method. Please implement the function `hyperparameter_search_settings` in `train_model.py` by returing the correct keyword arguments for `Trainer.hyperparameter_search`. (Observe that, at the end of `train_model.py`, these keyword arguments are passed to `Trainer.hyperparameter_search` via dictionary unpacking.)  \n",
    "\n",
    "Your code should support the following requirements.\n",
    "- Your hyperparameter tuning configuration must match your answer for Problem 1c.\n",
    "- You must use Optuna for hyperparameter tuning.\n",
    "- You must indicate to Optuna that the hyperparameter search should maximize accuracy.\n",
    "\n",
    "Please use the following resources for guidance.\n",
    "- [The Hugging Face tutorial on hyperparameter tuning](https://huggingface.co/docs/transformers/hpo_train)\n",
    "- [The documentation for `Trainer.hyperparameter_search`](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/trainer#transformers.Trainer.hyperparameter_search)\n",
    "- [The documentation for Optuna's `GridSampler`](https://optuna.readthedocs.io/en/v2.0.0/reference/generated/optuna.samplers.GridSampler.html)\n",
    "\n",
    "## Problem 3: Run Experiment (20 Points in Total)\n",
    "\n",
    "To complete the assignment, you will now run your code and report on the results. It is recommended that you run your code on [Google Colaboratory](https://colab.research.google.com/) using a free GPU.\n",
    "\n",
    "### Problem 3a: Train Models (Code and Written, 10 Points)\n",
    "\n",
    "Please now run the following experimental procedure by running `train_model.py` as a Python script:\n",
    "- first, fine-tune a BERT$_{\\text{tiny}}$ model on the IMDb dataset _with_ BitFit;\n",
    "- then, fine-tune a BERT$_{\\text{tiny}}$ model on the IMDb dataset _without_ BitFit.\n",
    "\n",
    "The `train_model.py` script should create a Pickle object containing information about the best hyperparameters found during hyperparameter tuning. Please submit this object, using the filenames `train_results_with_bitfit.p` and `train_results_without_bitfit.p` for your two training runs, respectively. Please also report the highest validation accuracy attained in each of your two training runs, as well as the hyperparameters used in those trials. Please format these results as a table such as the following.\n",
    "\n",
    "| | Validation Accuracy | Learning Rate | Batch Size |\n",
    "|---|---|---|---|\n",
    "| Without BitFit | | | |\n",
    "| With BitFit | | | |\n",
    "\n",
    "### Problem 3b: Test Models and Report Results (Code and Written, 10 Points)\n",
    "\n",
    "For each of your two training runs, please test the model that attained the best validation accuracy across all hyperparameter tuning trials. You may do so by running the `test_model.py` script. Once testing is complete, please report your results in the form of a table such as the following.\n",
    "\n",
    "| | # Trainable Parameters | Test Accuracy | \n",
    "|---|---|---|\n",
    "| Without BitFit | | | \n",
    "| With BitFit | | | \n",
    "\n",
    "The `test_model.py` script should create a Pickle object containing information about test results. Please submit this object, using the filenames `test_results_with_bitfit.p` and `test_results_without_bitfit.p` for your two tests.\n",
    "\n",
    "Finally, please comment on your results. How do they compare to the results reported by Zaken et al. (2020)? What does this say about BitFit and its applicability to other pre-trained Transformers?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
